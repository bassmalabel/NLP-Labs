{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58725ffb",
   "metadata": {},
   "source": [
    "TP2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e993a",
   "metadata": {},
   "source": [
    "EXO 1: Tokenization\n",
    "Objective: Understand the process of tokenization using spaCy and analyze token properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ef5cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google PROPN nsubj\n",
      "is AUX aux\n",
      "planning VERB ROOT\n",
      "to PART aux\n",
      "purchase VERB xcomp\n",
      "an DET det\n",
      "U.S. PROPN compound\n",
      "software NOUN compound\n",
      "company NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "120 NUM compound\n",
      "million NUM pobj\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "sentence= 'Google is planning to purchase an U.S. software company for $120 million'\n",
    "#load the model we're going to use for the tokenization 'English Core Web Small' \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#apply the tokenization\n",
    "doc = nlp(sentence)\n",
    "#access some meta data of the tokens (properties)\n",
    "for token in doc:\n",
    "  print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b7957ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The token is : Google\n",
      "The shape Xxxxx\n",
      "is_alpha tells if the token contains only  True\n",
      "is_stop False\n",
      "is_punct False\n",
      "is number False\n",
      "\n",
      " The token is : is\n",
      "The shape xx\n",
      "is_alpha tells if the token contains only  True\n",
      "is_stop True\n",
      "is_punct False\n",
      "is number False\n",
      "\n",
      " The token is : planning\n",
      "The shape xxxx\n",
      "is_alpha tells if the token contains only  True\n",
      "is_stop False\n",
      "is_punct False\n",
      "is number False\n",
      "\n",
      " The token is : to\n",
      "The shape xx\n",
      "is_alpha tells if the token contains only  True\n",
      "is_stop True\n",
      "is_punct False\n",
      "is number False\n",
      "\n",
      " The token is : purchase\n",
      "The shape xxxx\n",
      "is_alpha tells if the token contains only  True\n",
      "is_stop False\n",
      "is_punct False\n",
      "is number False\n",
      "\n",
      " The token is : an\n",
      "The shape xx\n",
      "is_alpha tells if the token contains only  True\n",
      "is_stop True\n",
      "is_punct False\n",
      "is number False\n",
      "\n",
      " The token is : U.S.\n",
      "The shape X.X.\n",
      "is_alpha tells if the token contains only  False\n",
      "is_stop False\n",
      "is_punct False\n",
      "is number False\n",
      "\n",
      " The token is : software\n",
      "The shape xxxx\n",
      "is_alpha tells if the token contains only  True\n",
      "is_stop False\n",
      "is_punct False\n",
      "is number False\n",
      "\n",
      " The token is : company\n",
      "The shape xxxx\n",
      "is_alpha tells if the token contains only  True\n",
      "is_stop False\n",
      "is_punct False\n",
      "is number False\n",
      "\n",
      " The token is : for\n",
      "The shape xxx\n",
      "is_alpha tells if the token contains only  True\n",
      "is_stop True\n",
      "is_punct False\n",
      "is number False\n",
      "\n",
      " The token is : $\n",
      "The shape $\n",
      "is_alpha tells if the token contains only  False\n",
      "is_stop False\n",
      "is_punct False\n",
      "is number False\n",
      "\n",
      " The token is : 120\n",
      "The shape ddd\n",
      "is_alpha tells if the token contains only  False\n",
      "is_stop False\n",
      "is_punct False\n",
      "is number True\n",
      "\n",
      " The token is : million\n",
      "The shape xxxx\n",
      "is_alpha tells if the token contains only  True\n",
      "is_stop False\n",
      "is_punct False\n",
      "is number True\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "  print('\\n The token is :', (token.text))\n",
    "  print('The shape', (token.shape_))\n",
    "  print('is_alpha tells if the token contains only ', (token.is_alpha))\n",
    "  print('is_stop', (token.is_stop))\n",
    "  print('is_punct', (token.is_punct))\n",
    "  print('is number', (token.like_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f0457",
   "metadata": {},
   "source": [
    "Comparing tokenizing using Regular Expression VS tokenizing using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02799a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'is', 'planning', 'to', 'purchase', 'an', 'U', 'S', 'software', 'company', 'for', '120', 'million']\n",
      "['Google', 'is', 'planning', 'to', 'purchase', 'an', 'U.S.', 'software', 'company', 'for', '$', '120', 'million']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "tokens = re.split(r'\\W+', sentence)\n",
    "tokens = [t for t in tokens if t]\n",
    "print(tokens)\n",
    "\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "print(spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d1dad",
   "metadata": {},
   "source": [
    "We can clearly notice that Spacy provides better results even on this small passage. \n",
    "This superiority is performance stems from spaCy’s advanced linguistic features and rule-based tokenization strategies, which are trained on large, high-quality corpora. \n",
    "Spacy has more complexe expressions that can handle mostly all types and forms of text, which is hard to do manually useing regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b228de92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/bishi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/bishi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ae5adc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'is', 'planning', 'to', 'purchase', 'an', 'U.S.', 'software', 'company', 'for', '$', '120', 'million']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk_tokens = word_tokenize(sentence)\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8ed20",
   "metadata": {},
   "source": [
    "We notice that both NLTK word tokenizer and spacy tokenizer perform similarily on this piece of text, this is due to the simplicity of this text. \n",
    "over all, the two libraries have similar accuracy when it comes to simple texts, however we may notice better results performed by Spacy when it comes to more complexe sentences. Because unlike simpler libraries (like nltk) that rely primarily on whitespace or punctuation-based splitting, spaCy integrates:\n",
    "1. part-of-speech tagging\n",
    "2. dependency parsing\n",
    "3. language-specific rules... \n",
    "to produce more accurate and context-aware tokens. As a result, its tokenization process captures the structure and meaning of text more effectively, leading to superior overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa70ce",
   "metadata": {},
   "source": [
    "Exo 2: Sentence Segmentation\n",
    "Objective: Understand the process of sentence segmentation using various NLP libraries and\n",
    "analyze different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14df74e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy sentence segmentation output:\n",
      "Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it.\n",
      "Did he mind?\n",
      "Adam Jones Jr. thinks he didn't.\n",
      "In any case, this isn't true...\n",
      "Well, with a probability of .9 it isn't.\n",
      "\n",
      "NLTK sentence segmentation output:\n",
      "Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e.\n",
      "he paid a lot for it.\n",
      "Did he mind?\n",
      "Adam Jones Jr. thinks he didn't.\n",
      "In any case, this isn't true... Well, with a probability of .9 it isn't.\n",
      "\n",
      "TextBlob sentence segmentation output:\n",
      "Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e.\n",
      "he paid a lot for it.\n",
      "Did he mind?\n",
      "Adam Jones Jr. thinks he didn't.\n",
      "In any case, this isn't true... Well, with a probability of .9 it isn't.\n"
     ]
    }
   ],
   "source": [
    "par = \"Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\"\n",
    "\n",
    "#use spaCy 'en_core_web_sm' model for sentence segmentation\n",
    "doc_2 = nlp(par)\n",
    "print('spaCy sentence segmentation output:')\n",
    "for sent in doc_2.sents:\n",
    "  print(sent)\n",
    "\n",
    "#use nltk for sentence segmentation\n",
    "nltp_sent_tokens = nltk.sent_tokenize(par)\n",
    "print('\\nNLTK sentence segmentation output:')\n",
    "for sent in nltp_sent_tokens:\n",
    "  print(sent)\n",
    "\n",
    "#use TextBlob for sentence segmentation \n",
    "from textblob import TextBlob\n",
    "print('\\nTextBlob sentence segmentation output:')\n",
    "blob = TextBlob(par)\n",
    "for sent in blob.sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54477d55",
   "metadata": {},
   "source": [
    "Comparing the outputs of each library: \n",
    "spaCy\n",
    "\t•\tMost robust and context-aware. Uses statistical models trained on large corpora, so it handles complex sentence boundaries more accurately.\n",
    "\t•\tHandles abbreviations well (e.g., “Mr.”, “i.e.”, “U.S.”) without incorrectly splitting sentences.\n",
    "\t•\tRecognizes ellipses and decimals (like “1.5” or “.9”) correctly.\n",
    "\t•\tBetter for real-world text, where punctuation can appear in many contexts.\n",
    "NLTK\n",
    "\t•\tRule-based segmentation using PunktSentenceTokenizer.\n",
    "\t•\tCan misinterpret abbreviations or decimals as sentence boundaries (e.g., splitting after “i.e.” or “Mr.” if not trained properly).\n",
    "\t•\tLess context-sensitive — relies heavily on punctuation and capitalization cues.\n",
    "\t•\tWorks fine for clean text, but struggles with informal writing or irregular punctuation.\n",
    "TextBlob\n",
    "\t•\tBuilt on top of NLTK’s Punkt tokenizer, so its segmentation behavior is nearly identical to NLTK’s.\n",
    "\t•\tShares the same limitations with abbreviations and ellipses. (but it provides slightly better accuracy)\n",
    "\t•\tEasier to use for quick tasks, but not ideal for nuanced sentence boundary detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "387fc090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Mr.\n",
      "2: Smith bought cheapsite.com for 1.5 million dollars, i.e.\n",
      "3: he paid a lot for it.\n",
      "4: Did he mind?\n",
      "5: Adam Jones Jr.\n",
      "6: thinks he didn't.\n",
      "7: In any case, this isn't true...\n",
      "8: Well, with a probability of .9 it isn't.\n"
     ]
    }
   ],
   "source": [
    "#implementing a simple rule base sentence segmentation function \n",
    "def sent_seg(text):\n",
    "    # Split text by '.', '?', or '!' followed by a space or end of line\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "sentences = sent_seg(par)\n",
    "for i, s in enumerate(sentences, 1):\n",
    "    print(f\"{i}: {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bda4af",
   "metadata": {},
   "source": [
    "We notice that: \n",
    "- The function splits after “Mr.” and “i.e.” and “Jr.” even though they’re not true sentence boundaries. That’s because the regex blindly treats any . followed by a space as the end of a sentence.\n",
    "- However for standard punctuation-based sentences, it works well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e979c5",
   "metadata": {},
   "source": [
    "Exo 4 + Homework: \n",
    "Apply and compare the out puts of the 3 libraries for each of the following: \n",
    "1. Part-of-Speech\n",
    "2. Stemming \n",
    "3. Lemmatization \n",
    "4. NER (Named Entity Recognition)\n",
    "5. Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d99dd487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/bishi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bishi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/bishi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/bishi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/bishi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/bishi/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /Users/bishi/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/bishi/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/bishi/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to /Users/bishi/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/bishi/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Core tokenizers & taggers\n",
    "nltk.download('punkt')\n",
    "#the corpus that has the rules of pos in nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Lemmatizer support\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Named Entity Recognition\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "\n",
    "# Corpora used by NLTK + TextBlob\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('movie_reviews')\n",
    "#the used sentence\n",
    "sent= 'The NLP system accurately classified 95% of the customer feedback as positive.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef186a",
   "metadata": {},
   "source": [
    "POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5cf5ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS with spaCy\n",
      "\n",
      "token: The\n",
      "POS: DET\n",
      "tag: DT\n",
      "Description og the tag:  determiner\n",
      "\n",
      "token: NLP\n",
      "POS: PROPN\n",
      "tag: NNP\n",
      "Description og the tag:  noun, proper singular\n",
      "\n",
      "token: system\n",
      "POS: NOUN\n",
      "tag: NN\n",
      "Description og the tag:  noun, singular or mass\n",
      "\n",
      "token: accurately\n",
      "POS: ADV\n",
      "tag: RB\n",
      "Description og the tag:  adverb\n",
      "\n",
      "token: classified\n",
      "POS: VERB\n",
      "tag: VBD\n",
      "Description og the tag:  verb, past tense\n",
      "\n",
      "token: 95\n",
      "POS: NUM\n",
      "tag: CD\n",
      "Description og the tag:  cardinal number\n",
      "\n",
      "token: %\n",
      "POS: NOUN\n",
      "tag: NN\n",
      "Description og the tag:  noun, singular or mass\n",
      "\n",
      "token: of\n",
      "POS: ADP\n",
      "tag: IN\n",
      "Description og the tag:  conjunction, subordinating or preposition\n",
      "\n",
      "token: the\n",
      "POS: DET\n",
      "tag: DT\n",
      "Description og the tag:  determiner\n",
      "\n",
      "token: customer\n",
      "POS: NOUN\n",
      "tag: NN\n",
      "Description og the tag:  noun, singular or mass\n",
      "\n",
      "token: feedback\n",
      "POS: NOUN\n",
      "tag: NN\n",
      "Description og the tag:  noun, singular or mass\n",
      "\n",
      "token: as\n",
      "POS: ADP\n",
      "tag: IN\n",
      "Description og the tag:  conjunction, subordinating or preposition\n",
      "\n",
      "token: positive\n",
      "POS: ADJ\n",
      "tag: JJ\n",
      "Description og the tag:  adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      "token: .\n",
      "POS: PUNCT\n",
      "tag: .\n",
      "Description og the tag:  punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "#spaCy\n",
    "doc = nlp(sent)\n",
    "print('POS with spaCy')\n",
    "for token in doc: \n",
    "    print('\\ntoken:', token)\n",
    "    print('POS:', token.pos_)\n",
    "    print('tag:', token.tag_)\n",
    "    print('Description og the tag: ', spacy.explain(token.tag_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1e2aebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS with NLTK\n",
      "Token: The  Tag: DT\n",
      "Token: NLP  Tag: NNP\n",
      "Token: system  Tag: NN\n",
      "Token: accurately  Tag: RB\n",
      "Token: classified  Tag: VBD\n",
      "Token: 95  Tag: CD\n",
      "Token: %  Tag: NN\n",
      "Token: of  Tag: IN\n",
      "Token: the  Tag: DT\n",
      "Token: customer  Tag: NN\n",
      "Token: feedback  Tag: NN\n",
      "Token: as  Tag: IN\n",
      "Token: positive  Tag: JJ\n",
      "Token: .  Tag: .\n"
     ]
    }
   ],
   "source": [
    "#NLTK\n",
    "print('POS with NLTK')\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "for token, tag in tags:\n",
    "    print(f\"Token: {token}  Tag: {tag}\")\n",
    "# NLTK doesn't have the explanation of the tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dac14eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS with TextBlob\n",
      "Token: The  Tag: DT\n",
      "Token: NLP  Tag: NNP\n",
      "Token: system  Tag: NN\n",
      "Token: accurately  Tag: RB\n",
      "Token: classified  Tag: VBD\n",
      "Token: 95  Tag: CD\n",
      "Token: %  Tag: NN\n",
      "Token: of  Tag: IN\n",
      "Token: the  Tag: DT\n",
      "Token: customer  Tag: NN\n",
      "Token: feedback  Tag: NN\n",
      "Token: as  Tag: IN\n",
      "Token: positive  Tag: JJ\n"
     ]
    }
   ],
   "source": [
    "#TextBlob \n",
    "print('POS with TextBlob')\n",
    "blob = TextBlob(sent)\n",
    "for token, tag in blob.tags:\n",
    "    print(f\"Token: {token}  Tag: {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a08148",
   "metadata": {},
   "source": [
    "Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63e6d000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with spaCy\n",
      "The → the\n",
      "NLP → NLP\n",
      "system → system\n",
      "accurately → accurately\n",
      "classified → classify\n",
      "95 → 95\n",
      "% → %\n",
      "of → of\n",
      "the → the\n",
      "customer → customer\n",
      "feedback → feedback\n",
      "as → as\n",
      "positive → positive\n",
      ". → .\n"
     ]
    }
   ],
   "source": [
    "#spaCy\n",
    "print('Lemmatization with spaCy')\n",
    "\n",
    "doc = nlp(sent)\n",
    "for token in doc:\n",
    "    print(token.text, \"→\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3706d7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with NLTK\n",
      "Original Tokens: ['The', 'NLP', 'system', 'accurately', 'classified', '95', '%', 'of', 'the', 'customer', 'feedback', 'as', 'positive', '.']\n",
      "Stemmed Tokens: ['the', 'nlp', 'system', 'accur', 'classifi', '95', '%', 'of', 'the', 'custom', 'feedback', 'as', 'posit', '.']\n",
      "Lemmatized Tokens: ['the', 'nlp', 'system', 'accurately', 'classify', '95', '%', 'of', 'the', 'customer', 'feedback', 'a', 'positive', '.']\n"
     ]
    }
   ],
   "source": [
    "#NLTK\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = word_tokenize(sent)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):  \n",
    "        return 'a'\n",
    "    elif tag.startswith('V'):  \n",
    "        return 'v'\n",
    "    elif tag.startswith('N'):  \n",
    "        return 'n'\n",
    "    elif tag.startswith('R'):  \n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n' \n",
    "\n",
    "stems = [stemmer.stem(word.lower(), get_wordnet_pos(tag)) for word, tag in tags]\n",
    "lemmas = [lemmatizer.lemmatize(word.lower(), get_wordnet_pos(tag)) for word, tag in tags]\n",
    "\n",
    "print('Lemmatization with NLTK')\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Stemmed Tokens:\", stems)\n",
    "print('Lemmatized Tokens:', lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83e482fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with TextBlob\n",
      "The             → The\n",
      "NLP             → NLP\n",
      "system          → system\n",
      "accurately      → accurately\n",
      "classified      → classified\n",
      "95              → 95\n",
      "of              → of\n",
      "the             → the\n",
      "customer        → customer\n",
      "feedback        → feedback\n",
      "as              → a\n",
      "positive        → positive\n"
     ]
    }
   ],
   "source": [
    "#TextBlob \n",
    "print('Lemmatization with TextBlob')\n",
    "blob = TextBlob(sent)\n",
    "for token in blob.words:\n",
    "    print(f\"{token:15} → {token.lemmatize()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeda1d3",
   "metadata": {},
   "source": [
    "NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c121ff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Named Entities:\n",
      "NLP                  → ORG\n",
      "95%                  → PERCENT\n"
     ]
    }
   ],
   "source": [
    "# spaCy\n",
    "print(\"spaCy Named Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:20} → {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0610162e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Named Entities:\n",
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  system/NN\n",
      "  accurately/RB\n",
      "  classified/VBD\n",
      "  95/CD\n",
      "  %/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  customer/NN\n",
      "  feedback/NN\n",
      "  as/IN\n",
      "  positive/JJ\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#NLTK\n",
    "print(\"NLTK Named Entities:\")\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "#we need to pass through pos first because nltk rules are based on \n",
    "tags = nltk.pos_tag(tokens)\n",
    "nltk_entities = nltk.ne_chunk(tags, binary=False)\n",
    "print(nltk_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6551f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob Named Entities:\n",
      "['nlp', 'customer feedback']\n"
     ]
    }
   ],
   "source": [
    "#TextBlob\n",
    "print(\"TextBlob Named Entities:\")\n",
    "print(blob.noun_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b96d44",
   "metadata": {},
   "source": [
    "Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9866e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Stop Words:\n",
      "['The', 'of', 'the', 'as']\n"
     ]
    }
   ],
   "source": [
    "# spaCy\n",
    "doc = nlp(sent)\n",
    "spacy_stopwords = [token.text for token in doc if token.is_stop]\n",
    "print(f\"spaCy Stop Words:\\n{spacy_stopwords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a5960b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK stopwords:\n",
      "['The', 'of', 'the', 'as']\n"
     ]
    }
   ],
   "source": [
    "#NLTK\n",
    "from nltk.corpus import stopwords\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "nltk_stops = set(stopwords.words('english'))\n",
    "sent_filtered = [w for w in tokens if w.lower() in nltk_stops]\n",
    "print(f\"NLTK stopwords:\\n{sent_filtered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e5d5122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob stop words Tokens :\n",
      "['The', 'of', 'the', 'as']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "blob = TextBlob(sent)\n",
    "textblob_stop_words = [word for word in blob.words if word.lower() in nltk_stops]\n",
    "print(f\"TextBlob stop words Tokens :\\n{textblob_stop_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beda628",
   "metadata": {},
   "source": [
    "We need to apply the three on more complexe and long sentences to really see the difference in preformance between the three libraries. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
